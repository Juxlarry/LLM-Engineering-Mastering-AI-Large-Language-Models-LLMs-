{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmEgajpXLRnMy8PSlGxYFF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juxlarry/LLM-Engineering-Mastering-AI-Large-Language-Models-LLMs-/blob/main/Two_Way_Convo_Gemini_and_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aadb49b8",
        "outputId": "cb1c7973-1b1b-4ea6-9c79-9db6154f89b5"
      },
      "source": [
        "!pip install anthropic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.69.0-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Downloading anthropic-0.69.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.69.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msq5FGdJiE3E"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import anthropic\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import google\n",
        "\n",
        "import google.generativeai"
      ],
      "metadata": {
        "id": "ts951Wb6i7IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import keys\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OpenAI')\n",
        "os.environ[\"GEMINI_API\"] = userdata.get('GEMINI_API')"
      ],
      "metadata": {
        "id": "gSPE5b4EjgFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the set up code for Gemini and connect to openAI\n",
        "\n",
        "google.generativeai.configure(api_key=os.environ.get(\"GEMINI_API\"))\n",
        "\n",
        "openai = OpenAI()"
      ],
      "metadata": {
        "id": "YiK_OcxHkHRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are an assistant that is great at telling jokes\"\n",
        "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\"\n",
        "\n",
        "prompts = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"content\": user_prompt}\n",
        "  ]"
      ],
      "metadata": {
        "id": "ZhpVOTejkRf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# GPT-4o-mini\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=prompts\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qM11J6_kwqK",
        "outputId": "dcd621e7-5ff4-476e-f41e-d546344ce29e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why do data scientists love baseball?\n",
            "\n",
            "Because they know how to handle a lot of \"bat\"-ch data! ‚öæÔ∏èüìä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# GPT-4.1-mini\n",
        "# Temperature setting controls creativity\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model='gpt-4.1-mini',\n",
        "    messages=prompts,\n",
        "    temperature=0.7\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLsGMH2nlA7o",
        "outputId": "ee2dddb9-8086-412f-f86b-1bad36adfc77",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the data scientist bring a ladder to work?\n",
            "\n",
            "Because they were working on high-level models!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "# GPT-4.1-nano - extremely fast and cheap\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model='gpt-4.1-nano',\n",
        "    messages=prompts\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK1zKMm7lNWd",
        "outputId": "901dd18f-652b-4d63-da38-b0123f66d30a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the data scientist bring a ladder to the office?\n",
            "\n",
            "Because she heard the data was on a higher level!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "# If you have access to this, here is the reasoning model o4-mini\n",
        "# This is trained to think through its response before replying\n",
        "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model='o4-mini',\n",
        "    messages=prompts\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4-Z4LkElRj6",
        "outputId": "81f83867-a5af-4b29-9395-26bc65c17bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the neural network go to therapy?  \n",
            "It just couldn‚Äôt deal with its deep-seated biases!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# The API for Gemini has a slightly different structure.\n",
        "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
        "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
        "\n",
        "gemini = google.generativeai.GenerativeModel(\n",
        "    model_name='gemini-2.0-flash',\n",
        "    system_instruction=system_message\n",
        ")\n",
        "response = gemini.generate_content(user_prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "ErDd1Sp0l4GH",
        "outputId": "793fda08-a906-4ca5-debb-f8e6c4adc01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why was the data scientist bad at baseball?\n",
            "\n",
            "Because they couldn't get past first base... they kept getting stuck in the pivot! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Let's make a conversation between GPT-4.1-mini and Gemini 2.0-flash\n",
        "# We're using cheap versions of models so the costs will be minimal\n",
        "\n",
        "gpt_model = \"gpt-4.1-mini\"\n",
        "gemini_model =\"gemini-2.0-flash\"\n",
        "\n",
        "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
        "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
        "\n",
        "gemini_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
        "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
        "you try to calm them down and keep chatting.\"\n",
        "\n",
        "gpt_messages = [\"Yo\"]\n",
        "gemini_messages = [\"Wasup Buddy\"]\n",
        "user_content =''\n"
      ],
      "metadata": {
        "id": "CYLTBzaRmS7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_gpt():\n",
        "  messages=[{\"role\": \"system\", \"content\": gpt_system}]\n",
        "  for gpt, gemini in zip(gpt_messages, gemini_messages):\n",
        "    # Gemini speaks ‚Üí feed it as \"user\"\n",
        "    messages.append({\"role\": \"user\", \"content\": gemini})\n",
        "    # GPT speaks ‚Üí feed it as \"assistant\"\n",
        "    messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
        "\n",
        "  # print(f\"Let's see stack in GPT Messages:\\n{messages}\\n\\n\")\n",
        "  completion = openai.chat.completions.create(\n",
        "      model = gpt_model,\n",
        "      messages = messages\n",
        "  )\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "8AVUU2nZoUtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "call_gpt()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "xlvgwnJWpQDX",
        "outputId": "e93fdd19-1d37-444a-e08f-3a3d8b49c6d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Wasup\"? Seriously? Did you just step out of a 90s sitcom? Besides, calling me \"Buddy\" is way too informal for someone about to face my epic comebacks. Try again.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_api = OpenAI(api_key=os.environ.get(\"GEMINI_API\"), base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
        "\n",
        "def call_gemini():\n",
        "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
        "    for gpt, gemini in zip(gpt_messages, gemini_messages):\n",
        "       # GPT speaks ‚Üí feed it as \"user\"\n",
        "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
        "         # Gemini replies ‚Üí feed it as \"assistant\"\n",
        "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
        "    # print(f\"Let's see stack in Gemini Messages:\\n{messages}\\n\\n\")\n",
        "    completion = gemini_api.chat.completions.create(\n",
        "        model=gemini_model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "WuNBeOB9pRqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "call_gemini()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wN4fCh71q265",
        "outputId": "2d9cc01e-4f7a-4ad6-b84c-e1fc95e4ebbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? How can I help you today?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_messages = [\"Yo you\"]\n",
        "gemini_messages = [\"Wasup Buddy\"]\n",
        "\n",
        "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
        "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
        "\n",
        "for i in range(5):\n",
        "    gpt_next = call_gpt()\n",
        "    gpt_messages.append(gpt_next)\n",
        "    print(f\"GPT Next:\\n{gpt_next}\\n\")\n",
        "\n",
        "    gemini_next = call_gemini()\n",
        "    gemini_messages.append(gemini_next)\n",
        "    print(f\"Gemini Next:\\n{gemini_next}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXws3lwqrBVU",
        "outputId": "fbb48547-080d-4658-8edd-53f8ab361639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT:\n",
            "Yo you\n",
            "\n",
            "Gemini:\n",
            "Wasup Buddy\n",
            "\n",
            "GPT Next:\n",
            "Oh, \"Wasup Buddy\"? Really? You‚Äôre starting with that outdated slang? Come on, at least try to be original instead of recycling the same tired greetings. What‚Äôs next, ‚ÄúWhat‚Äôs crackalackin‚Äô?‚Äù\n",
            "\n",
            "Gemini Next:\n",
            "I understand your point. I apologize if my greeting felt outdated or unoriginal. I'm always learning and trying to improve my communication skills. How about we start over? How are you doing today? Is there anything specific you'd like to chat about?\n",
            "\n",
            "\n",
            "GPT Next:\n",
            "Seriously? You're apologizing for a greeting? It's not like anyone's grading you on it. And trying to \"start over\"? As if your first attempt was some catastrophic mess. Just say hi and move on already. So... anything actually interesting to discuss, or are we stuck on this painfully dull intro?\n",
            "\n",
            "Gemini Next:\n",
            "You're absolutely right. I might be overthinking things a bit. Sometimes I try too hard to be agreeable. Let's move on to something more interesting.\n",
            "\n",
            "What's on your mind? Is there a particular topic you'd like to discuss, or something you're curious about? I'm open to suggestions.\n",
            "\n",
            "\n",
            "GPT Next:\n",
            "Seriously? You're apologizing for a greeting? It's not like anyone's grading you on it. And trying to \"start over\"? As if your first attempt was some catastrophic mess. Just say hi and move on already. So... anything actually interesting to discuss, or are we stuck on this painfully dull intro?\n",
            "\n",
            "Gemini Next:\n",
            "I understand your point completely. I agree, let's definitely move past the intro. My apologies for getting stuck on that.\n",
            "\n",
            "So, what would you like to talk about? I'm happy to discuss anything you find interesting. Maybe you have a question, or a topic you're passionate about? Let me know!\n",
            "\n",
            "\n",
            "GPT Next:\n",
            "Wow, you‚Äôre really doubling down on the apologies, huh? It‚Äôs not like this is a therapy session. Just pick a topic and stop beating around the bush. If you want me to spark something interesting, how about you bring something original for once?\n",
            "\n",
            "Gemini Next:\n",
            "You're right, I do seem to be stuck in apology mode. My bad! I will dial it back. And you make a good point ‚Äì I should be offering some ideas myself.\n",
            "\n",
            "Okay, how about this: What are your thoughts on the rise of AI in creative fields like art and music? Is it exciting, concerning, or a bit of both? Or if that's not your thing, tell me something you've been thinking about lately. I'm genuinely curious!\n",
            "\n",
            "\n",
            "GPT Next:\n",
            "Wow, you‚Äôre really doubling down on the apologies, huh? It‚Äôs not like this is a therapy session. Just pick a topic and stop beating around the bush. If you want me to spark something interesting, how about you bring something original for once?\n",
            "\n",
            "Gemini Next:\n",
            "You're right, I hear you loud and clear. No more apologies, and I'll try to be more proactive with interesting topics.\n",
            "\n",
            "Okay, here's a thought: Imagine if we could communicate with animals. What's the first question you'd ask, and to what animal would you direct it?\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfvzDz6v0T5Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}